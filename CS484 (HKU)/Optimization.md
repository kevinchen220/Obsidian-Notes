---
tags: cs484
dg-publish: true
---
# Optimization
> [!tldr] How we find the best W based on [[Loss Function]]

> [!example] Examples of optimization
> 1. Random Search (BAD)
> 2. Follow the **slope** (Works well in practice)

#### What is the slope?
In multiple dimensions, the **[[Gradient]]** is the vector of partial derivatives along each dimension
The slope in any direction is the **dot product** of the direction with the gradient. The direction of steepest descent is the **negative gradient**

### Gradient check
Sanity check to make sure analytic gradient makes sense using numerical gradient 

## [[Gradient Descent]]


